\documentclass[a4paper]{article}
% \usepackage[margin=1.25in]{geometry}
\usepackage[inner=2.0cm,outer=2.0cm,top=2.5cm,bottom=2.5cm]{geometry}
% \usepackage{ctex}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{algorithm}
\usepackage{algorithmic} 


\newcommand{\homework}[5]{
    \pagestyle{myheadings}
    \thispagestyle{plain}
    \newpage
    \setcounter{page}{1}
    \noindent
    \begin{center}
    \framebox{
        \vbox{\vspace{2mm}
        \hbox to 6.28in { {\bf Optimization Methods \hfill #2} }
        \vspace{6mm}
        \hbox to 6.28in { {\Large \hfill #1 \hfill} }
        \vspace{6mm}
        \hbox to 6.28in { {\it Instructor: {\rm #3} \hfill Name: {\rm #4}, StudentId: {\rm #5}}}
        \vspace{2mm}}
    }
    \end{center}
    % \markboth{#4 -- #1}{#4 -- #1}
    \vspace*{4mm}
}

\renewcommand{\algorithmicrequire}{\textbf{Input:}} 
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newenvironment{solution}
{\color{blue} \paragraph{Solution.\\}}
{\newline \qed}

\begin{document}
%==========================Put your name and id here==========================
\homework{Homework 3}{Fall 2019}{Lijun Zhang}{Li Weikang}{181220031}

\paragraph{Notice}
\begin{itemize}
    \item The submission email is: \textbf{njuoptfall2019@163.com}.
    \item Please use the provided \LaTeX{} file as a template. If you are not familiar with \LaTeX{}, you can also use Word to generate a \textbf{PDF} file.
\end{itemize}
\paragraph{Problem 1: Equality Constrained Least-squares}
~\\
Consider the equality constrained least-squares problem
\begin{gather*}
\begin{matrix}
\text{minimize~~} & \frac{1}{2}\|Ax-b\|_2^2\\
\text{subject to} & Gx=h~~\\
\end{matrix}
\end{gather*}
where $A\in\mathbf{R}^{m\times n}$ with $\mathbf{rank}~A=n$, and $G\in\mathbf{R}^{p\times n}$ with $\mathbf{rank}~G=p$.
\begin{enumerate}[a)]
    \item Derive the Lagrange dual problem with Lagrange multiplier vector $v$.
    \item Derive expressions for the primal solution $x^\star$ and the dual solution $v^\star$.
\end{enumerate}
\begin{solution}
a) It is easy to see that the Lagrangian is
\begin{equation}\nonumber
\begin{aligned}
	\mathcal{L}(x,v) \ &=\  \frac{1}{2}\|Ax-b\|_2^2+v^T(Gx-h)\\ \ &=\ \frac{1}{2} x^TA^TAx + (G^Tv - A^Tb)^Tx - v^Th + \frac{1}{2} b^Tb,
\end{aligned}
\end{equation}
with minimizer $x=-(A^TA)^{-1}(G^Tv-A^Tb)$. Accordingly the dual function is $$g(v)=-\frac{1}{2}(G^Tv-2A^Tb)^T(A^TA)^{-1}(G^Tv-2A^Tb)-v^Th+\frac{1}{2}b^Tb$$ 
Therefore, the Lagrange dual problem can be described as
\begin{gather*}
\begin{matrix}
\text{maximize~~} & -\frac{1}{2}(G^Tv-2A^Tb)^T(A^TA)^{-1}(G^Tv-2A^Tb)-v^Th+\frac{1}{2}b^Tb
\end{matrix}
\end{gather*}
\quad b) The KKT optimality conditions are $$A^T(Ax^*-b)+G^Tv^*=0,\quad Gx^*=h.$$
From the first equation, $$x^*=(A^TA)^{-1}(A^Tb-G^Tv^*).$$ Plugging this expression for $x^*$ into the second equation gives $$G(A^TA)^{-1}A^Tb-G(A^TA)^{-1}G^Tv^*=h$$ $i.e.,$ $$v^*=-(G(A^TA)^{-1}G^T)^{-1}(h-G(A^TA)^{-1}A^Tb).$$ Substituting in the first expression gives an analytical expression for $x^*$.
\end{solution}

\paragraph{Problem 2: Support Vector Machines}
~\\
Consider the following optimization problem
\begin{gather*}
\begin{matrix}
\text{minimize~~} & \sum_{i=1}^n\max\left(0,1-y_i(w^Tx_i+b)\right)+\frac{\lambda}{2}\|w\|_2^2\\
\end{matrix}
\end{gather*}
where $x_i\in\mathbf{R}^{d},y_i\in \mathbf{R},i=1,\cdots,n$ are given, and $w\in \mathbf{R}^d,b\in\mathbf{R}$ are the variables.
\begin{enumerate}[a)]
    \item Derive an equivalent problem by introducing new variables $u_i,i=1,\cdots,n$ and equality constraints \[u_i=y_i(w^Tx_i+b),i=1,\cdots,n.\]
    \item Derive the Lagrange dual problem of the above equivalent problem.
    \item Give the Karush-Kuhn-Tucker conditions.
\end{enumerate}

\noindent\emph{Hint: Let $\ell(x)=\max(0,1-x)$. Its conjugate function $\ell^\ast(y)=\sup\limits_{x}(yx-\ell(x))=\left\{
\begin{aligned}
&y, \quad-1\leq y\leq0 \\
&\infty,\quad\text{otherwise}
\end{aligned}
\right.$}
\begin{solution}
a) We plug the equality constraints into the original problem to derive the equivalent problem
\begin{gather*}
\begin{matrix}
\text{minimize~~} &\sum_{i=1}^n\max\left(0,1-u_i\right)+\frac{\lambda}{2}\|w\|_2^2\\
\text{subject to} &u_i=y_i(w^Tx_i+b),\quad i=1,\cdots,n.~~\\
\end{matrix}
\end{gather*}
\quad b) From the equivalent problem above we derive its Lagrangian
$$\mathcal{L}(w,b,u,v)=\sum_{i=1}^n\max\left(0,1-u_i\right)+\frac{\lambda}{2}\|w\|_2^2+\sum_{i=1}^nv_i(u_i-y_i(w^Tx_i+b))$$ 
Thus, the Lagrangian dual function is 
\begin{equation}\nonumber
\begin{aligned}
g(v)&=\inf_{w,b,u}\sum_{i=1}^n\max\left(0,1-u_i\right)+\frac{\lambda}{2}\|w\|_2^2+\sum_{i=1}^nv_i(u_i-y_i(w^Tx_i+b))\\ &=\ \inf_{u}\sum_{i=1}^n\left[\max\left(0,1-u_i\right)+v_iu_i\right]+\frac{1}{2\lambda}\|\sum_{i=1}^nv_ix_iy_i\|^2-\sum_{i=1}^n\frac{v_iy_i}{\lambda}\sum_{j=1}^nv_jx_j^Tx_i2y_j
\end{aligned}
\end{equation}
Here we introduce $\ell(x)=\max(0,1-x)$. \\And its conjugate function $\ell^\ast(y)=\sup\limits_{x}(yx-\ell(x))=\left\{
\begin{aligned}
&y, \quad-1\leq y\leq0 \\
&\infty,\quad\text{otherwise}
\end{aligned}
\right.$\\ 
Therefore,
\begin{equation}\nonumber
\begin{aligned}
g(v) &= \inf_{u}\sum_{i=1}^n\left[\ell(u_i)+v_iu_i\right]+\frac{1}{2\lambda}\|\sum_{i=1}^nv_ix_iy_i\|^2-\sum_{i=1}^n\frac{v_iy_i}{\lambda}\sum_{j=1}^nv_jx_j^Tx_iy_j\\ &= -\sup_u\sum_{i=1}^n\left[-v_iu_i-\ell(u_i)\right]+\frac{1}{2\lambda}\|\sum_{i=1}^nv_ix_iy_i\|^2-\sum_{i=1}^n\frac{v_iy_i}{\lambda}\sum_{j=1}^nv_jx_j^Tx_iy_j\\ &= -\sum_{i=1}^n\sup_u\left[-v_iu_i-\ell(u_i)\right]+\frac{1}{2\lambda}\|\sum_{i=1}^nv_ix_iy_i\|^2-\sum_{i=1}^n\frac{v_iy_i}{\lambda}\sum_{j=1}^nv_jx_j^Tx_iy_j\\ &= \sum_{i=1}^nv_i+\frac{1}{2\lambda}\|\sum_{i=1}^nv_ix_iy_i\|^2-\sum_{i=1}^n\frac{v_iy_i}{\lambda}\sum_{j=1}^nv_jx_j^Tx_iy_j\\
\end{aligned}
\end{equation}
where $0 \leq v_i \leq 1.$\\
Finally, the Lagrangian dual problem is
\begin{gather*}
\begin{matrix}
\text{maximize~~} &\sum_{i=1}^nv_i+\frac{1}{2\lambda}\|\sum_{i=1}^nv_ix_iy_i\|^2-\sum_{i=1}^n\frac{v_iy_i}{\lambda}\sum_{j=1}^nv_jx_j^Tx_iy_j\\
\text{subject to} &\sum_{i=1}^nv_iy_i=0,\\&0 \leq v_i \leq 1.~~\\
\end{matrix}
\end{gather*}
\quad c) The KKT optimality conditions are:
$$u_i^*=y_i({w^*}^Tx_i+b^*),i=1,\cdots,n.$$
$$\nabla\sum_{i=1}^n\max\left(0,1-u_i^*\right)+\frac{\lambda}{2}\|w^*\|_2^2+\nabla\sum_{i=1}^nv_i(u_i^*-y_i({w^*}^Tx_i+b^*))=0.$$
\end{solution}

\paragraph{Problem 3: Euclidean Projection onto the Simplex}
~\\
Consider the following optimization problem
\begin{gather*}
\begin{matrix}
\text{minimize~~} & \frac{1}{2}\left\|y-x\right\|^2_2\quad\\
\text{subject to} & \mathbf{1}^Ty=r\quad\quad\\
&y\succeq0\quad\quad\quad
\end{matrix}
\end{gather*}
where $r>0$, $x\in \mathbb{R}^n$ is given, and $y\in \mathbf{R}^n$ is the variable. Give an algorithm to solve this problem and prove the correctness of your algorithm.

\noindent\emph{Hint: Derive the Lagrangian of this problem and apply the Karush-Kuhn-Tucker conditions. If you need more hints, please read the following paper \cite{Wang2013}}
\begin{solution}
	To begin with, we put forward an efficient algorithm to solve the problem.
\begin{algorithm}[!h]
	\caption{Euclidean projection of a vector onto the probability simplex.}
	\begin{algorithmic} \color{blue}
		\REQUIRE $\textbf{x} \in \mathbb{R}^n$		
		\STATE Sort \textbf{x} into \textbf{u}: $u_1 \geq u_2 \geq \cdots \geq u_n$
		\STATE Find $\rho$ = $\max\{1 \leq j \leq n: u_j+\frac{1}{j}(r - \sum_{i=1}^ju_i)>0\}$ 
		\STATE Define $\lambda=\frac{1}{\rho}(r - \sum_{i=1}^\rho u_i)$
		\ENSURE $\textbf{y}\ \mathrm{s.t.}\ y_i = \max\{x_i+\lambda, 0\},i=1,\dots ,n.$
	\end{algorithmic}
\end{algorithm}\\
$\textit{Proof.}$ The Lagrangian of the problem is 
	$$\mathcal{L}(\mathbf{y},\lambda,\beta)=\frac{1}{2}\|y-x\|_2^2-\lambda(\mathbf{1}^Ty-r)-\beta^Ty$$ where $\lambda$ and $\mathbf{\beta}=[\beta_1,\dots,\beta_n]^T$ are the Lagrange multipliers for the constraints. At the optimal solution $\mathbf{y}$ the following KKT conditions hold:
\begin{align}
	y_i-x_i-\lambda-\beta_i &= 0,\qquad i = 1,\dots,n\\
	y_i & \geq 0,\qquad i = 1,\dots,n\\
	\beta_i &\geq 0,\qquad i = 1,\dots,n\\ 
	y_i\beta_i &= 0,\qquad i = 1,\dots,n\\
	\sum_{i=1}^ny_i &= r.
\end{align}
From the condition (4), it is trivial that if $y_i > 0$, we must have $\beta_i=0$ and $y_i=x_i+\lambda$; if $y_i = 0$, we must have $\beta_i \geq 0$ and $y_i=x_i+\lambda+\beta_i=0$, thus $x_i+\lambda=-\beta_i \leq 0$. It is obvious that the components of the optimal solution $\textbf{y}$ that are zeros correspond to the smaller components of $\textbf{x}$. Without loss of generality, we assume the components of $\textbf{x}$ are sorted and $\textbf{y}$ uses the same ordering, i.e.,
\begin{equation}\nonumber
\begin{aligned}
	&x_1\geq \cdots \geq x_\rho \geq x_\rho+1 \geq \cdots \geq x_n,
	\\&y_1\geq \cdots \geq y_\rho > y_\rho+1 = \cdots = y_n,
\end{aligned}
\end{equation}
and that $y_1\geq \cdots \geq y_\rho > 0$, $y_\rho+1 = \cdots = y_n=0$ In other words, $\rho$ is the number of positive components in the solution $\textbf{y}$. Now we apply the last condition and have
$$r=\sum_{i=1}^ny_i=\sum_{i=1}^\rho y_i=\sum_{i=1}^\rho (x_i+\lambda)$$
which gives $\lambda=\frac{1}{\rho}\left(r-\sum_{i=1}^\rho x_i \right)$. Hence $\rho$ is the key to the solution. And once we know $\rho$(there are only $n$ possible values of it), we can compute $\lambda$ and the optimal solution is obtained by just adding $\lambda$ to each component of $\textbf{x}$ and thresholding as in the end of Algorithm 1.(It is easy to check that this solution indeed satisfies all KKT conditions.) In the algorithm, we carry out the tests for $j=1,\dots,n$ if $t_j=x_j+\frac{1}{j}\left(r-\sum_{i=1}^j x_i\right)>0 $ We now prove that the number of times this test turns out positive is exactly $\rho$. The following theorem is essentially Lemma 3 of Shalev-Shwartz and Singer (2006).\\
$\textit{Theorem }$1. Let $\rho$ be the number of positive components in the solution \textbf{y}, then 
$$\rho=\max\{1\leq j\leq n: x_j+\frac{1}{j}(r-\sum_{i=1}^j x_i)>0\}.$$
$\textit{Proof.}$ From the KKT condition (2) we have that $\lambda\rho=\left(r-\sum_{i=1}^\rho x_i \right)$, $x_i+\lambda >0$ for $i=1,\dots,\rho$ and $x_i+\lambda \leq 0$ for $i=\rho+1,\dots,n.$ In the sequel, we know that for $j=1,\dots,n,$ the test will continue to be positive until $j=\rho$ and then stay non-positive afterwards, i.e., $x_j+\frac{1}{j}\left(r-\sum_{i=1}^j x_i\right)>0$ for $j\leq\rho$, and $x_j+\frac{1}{j}\left(r-\sum_{i=1}^j x_i\right)\leq 0$ for $j>\rho.$
\begin{enumerate} [(i)]
\item For $j=\rho$, we have
$$x_\rho+\frac{1}{\rho}\left(r-\sum_{i=1}^\rho x_i\right)=x_\rho+\lambda=y_\rho>0$$
\item For $j<\rho$, we have 
\begin{equation}\nonumber
\begin{aligned}
x_j+\frac{1}{j}\left(r-\sum_{i=1}^j x_i\right)=\frac{1}{j}(jx	_j+r-\sum_{i=1}^j x_i)=\frac{1}{j}\left(jx_j+\sum_{i=j+1}^\rho x_i+r-\sum_{i=1}^\rho x_i\right)=\frac{1}{j}\left(jx_j+\sum_{i=j+1}^\rho x_i+\rho\lambda\right) \\ =\frac{1}{j}\left(j(x_j+\lambda)+\sum_{i=j+1}^\rho(x_i+\lambda)\right).
\end{aligned}
\end{equation}
Since $x_i+\lambda>0$ for $i=j,\dots,\rho,$ we have $x_j+\frac{1}{j}(r-\sum_{i=1}^jx_i)>0.$
\item For $j>\rho ,$ we have
\begin{equation}\nonumber
\begin{aligned}
x_j+\frac{1}{j}\left(r-\sum_{i=1}^j x_i\right)=\frac{1}{j}(jx_j+r-\sum_{i=1}^j x_i)=\frac{1}{j}\left(jx_j+r-\sum_{i=1}^\rho x_i-\sum_{i=\rho +1}^j x_i\right)=\frac{1}{j}\left(jx_j+\rho\lambda-\sum_{i=\rho+1}^j x_i\right) \\ =\frac{1}{j}\left(\rho(x_j+\lambda)+\sum_{i=\rho+1}^j(x_j-x_i)\right).
\end{aligned}
\end{equation}
\end{enumerate}
\qquad Notice that $x_j+\lambda\leq 0$ for $j>\rho$, and $x_j\leq x_i$ for $j\geq i$ since $\textbf{x}$ is sorted, therefore $x_j+\frac{1}{j}(1-\sum_{i=1}^jx_i)<0.$
\end{solution}

\paragraph{Problem 4: Optimality Conditions}
~\\
Consider the problem
\begin{equation*}
    \begin{split}
        &\text{minimize~~} \quad  \text{tr}(2X) - \log{\det{(3X)}} \\
        &\text{subject to} \quad~  2Xs=y
    \end{split}
\end{equation*}
with variable $X \in \mathbf{S}^n$ and domain $\mathbf{S}_{++}^n$. Here, $y \in \mathbf{R}^n$ and $s \in \mathbf{R}^n$ are given, with $s^T y =1$.
\begin{enumerate}[a)]
    \item Give the Lagrange and then derive the Karush-Kuhn-Tucker conditions.
    \item Verify that the optimal solution is given by
\

    \begin{equation*}
        X^\star = \frac{1}{2} \left(I + yy^T - \frac{ss^T}{s^T s}\right) .
    \end{equation*}
\end{enumerate}

\begin{solution}
We introduce a Lagrange multiplier $z \in \mathbf{R}^n$ for the equality constraint.\\
According to the properties of \textbf{trace}, $\nabla_A\text{tr}(AB)=\nabla_A\text{tr}(BA)=B^T.$ \\Thus, we have:
$$\nabla_X\text{tr}(nX)=n\nabla_X\text{tr}(IX)=nI$$
Refer to the proof in \textbf{section A.4.1} of the book Stephen Boyd, Lieven Vandenberghe, Convex Optimization, we have:
$$\nabla_X\text{log det} X=X^{-1}$$	
The KKT optimality conditions are: 
$$X \succ 0,\quad 2Xs=y, \quad X^{-1}=2I+zs^T+sz^T.\eqno{(1)}$$ 
We first determine $z$ from the condition $2Xs=y.$ Multiplying the gradient equation on the right with y gives 
$$s=\frac{1}{2}X^{-1}y=y+\frac{1}{2}(z+(z^Ty)s).\eqno{(2)}$$
By taking the inner product with y on both sides and simplifying, we get $z^Ty=1-y^Ty.$\\ Substituting in (2) we get $$z=-2y+(1+y^Ty)s,$$ and substitute this expression for $z$ in (1) gives 
\begin{equation}\nonumber
\begin{aligned}
X^{-1} \ &= \ 2(I -ys^T-sy^T+(1+y^Ty)ss^T)
\end{aligned}\
\end{equation}
Finally we verify that this inverse of the matrix $X^*$ given above:
\begin{equation}\nonumber
\begin{aligned}
2(I -ys^T-sy^T+(1+y^Ty)ss^T)X^* &= \ (I+yy^T-(1/s^Ts)ss^T)+(1+y^Ty)(ss^T+sy^T-ss^T)\\ &\quad \  -(ys^T+yy^T-ys^T)-(sy^T+(y^Ty)sy^T-(1/s^Ts)ss^T)\\ &= \ I
\end{aligned}
\end{equation}
To complete the solution, we prove that $X^* \succ 0.$ An easy way to see this is to note that  $$X^\star = \frac{1}{2} \left(I + yy^T - \frac{ss^T}{s^T s}\right) = \frac{1}{2}\left(I+ \frac{ys^T}{\|s\|_2}-\frac{ss^T}{s^Ts}\right)\left(I+ \frac{ys^T}{\|s\|_2}-\frac{ss^T}{s^Ts}\right)^T.$$
\end{solution}


\begin{thebibliography}{1}

\bibitem{Wang2013}
Weiran Wang, and Miguel \'{A}. Carreira-Peroi\~{n}\'{a}n.
\newblock Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application.
\newblock \emph{arXiv:1309.1541}, 2013.

\end{thebibliography}

\end{document}
